{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5zYLAE8LTvr"
   },
   "source": [
    "**Instituto Tecnológico de Costa Rica**\n",
    "\n",
    "**Escuela de Ingeniería en Computación**\n",
    "\n",
    "**Curso: Inteligencia Artificial**\n",
    "\n",
    "**Segundo Semestre 2021**\n",
    "\n",
    "**Profesor: Luis-Alexander Calvo-Valverde**\n",
    "\n",
    "**Trabajo Práctico:** 04\n",
    "\n",
    "**Datos de la entrega:** Miércoles 27 de octubre 2021, a más tardar a las 11:59 pm\n",
    "\n",
    "**Estudiantes:**\n",
    "- Eduardo Madrigal Marín\n",
    "- Gabriel Vargas Rodríguez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Nota Introductoria\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "0bn7ujBQAqxw",
    "outputId": "9b4cccb3-5be5-465c-ed78-9580132bdb5d"
   },
   "source": [
    "Ustedes han sido contratados por la empresa FUTURA para trabajar con dos conjuntos de datos y lograr la mejor predicción posible, dadas ciertas métricas.\n",
    "\n",
    "Para efectos de dudas o ampliaciones sobre el proyecto, además de lo que se indique en este cuaderno, considere a Luis-Alexánder Calvo-Valverde como su cliente para atender sus consultas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Parte 1  -  Clasificación (50 puntos)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considere lo siguiente:\n",
    "1. Conjunto de datos: mnist_ALL.csv\n",
    "1. Los atributos corresponden a una imagen de 28x28.\n",
    "1. El atributo a predecir es label.\n",
    "1. Proponga al menos tres algoritmos a utilizar\n",
    "1. Métricas:\n",
    "    1. Accuracy\n",
    "    1. f1_score\n",
    "1. Debe presentar en este cuaderno:\n",
    "    1. Pre-procesamiento de los datos, explicando las decisiones en cada caso. Observará que en este caso viene bastante preparado\n",
    "    1. Para cada uno de los algoritmos seleccionados: \n",
    "        1. Explicación del algoritmo.\n",
    "        1. Explicación de la implementación seleccionada y de sus parámetros.\n",
    "    1. Explicación del diseño experimental por ejecutar. Recuerde que si el algoritmo requiere seleccionar hyperparámetros, hay que dividir en tres conjuntos de datos: Training, Validation, Testing (60%, 20%, 20%). Se le recomienda confirmar con el profesor cuántos y cuáles hyperparámetros validar.\n",
    "    1. Programación del diseño experimental. \n",
    "    1. Tablas de resultados, gráficos y conclusiones de los resultados, recomendando a su contratante cuál algoritmo utilizar, con qué configuración y por qué lo recomienda. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "First, we load and study the dataset. In this step, we look for missing values, outliers and any other information that may affect the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
      "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
      "\n",
      "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
      "0      0      0      0      0      0      0      0      0  \n",
      "1      0      0      0      0      0      0      0      0  \n",
      "2      0      0      0      0      0      0      0      0  \n",
      "3      0      0      0      0      0      0      0      0  \n",
      "4      0      0      0      0      0      0      0      0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "# loads file as a DataFrame\n",
    "data = pd.read_csv('csv/mnist_ALL.csv', header = 0, sep=';') # index_col = 0\n",
    "print(data.head(5)) # shows first 5 registers to check if everything is loaded correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily determine that the dataset has no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().mean().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also observe that there are no outliers, since all of the values for each variable are within the expected range. According to the client, the range of values for each of the variables is between 0 and 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.columns:\n",
    "    if not data[i].between(0,255).all():\n",
    "        print(f'{i}: {data[i].min()}, {data[i].max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can determine the different categories in which we are going to classify the data, along with the amount of entries for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    6903\n",
       "1    7877\n",
       "2    6990\n",
       "3    7141\n",
       "4    6824\n",
       "5    6313\n",
       "6    6876\n",
       "7    7293\n",
       "8    6825\n",
       "9    6958\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('label').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Separation + Feature Selection\n",
    "\n",
    "After studying the dataset, we have determined that the data meets all of the requirements for analysis with the selected classification models. \n",
    "\n",
    "Before proceeding with the model comparison and selection, we must first divide the dataset into train (validation) and test sets. Once the split is carried out, we can also filter some of the dataset's features, since not all of them necessarily provide valuable info to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>1x10</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47339</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67456</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12308</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  1x10  ...  28x19  28x20  \\\n",
       "47339    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "67456    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "12308    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "32557    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "664      0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "\n",
       "       28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "47339      0      0      0      0      0      0      0      0  \n",
       "67456      0      0      0      0      0      0      0      0  \n",
       "12308      0      0      0      0      0      0      0      0  \n",
       "32557      0      0      0      0      0      0      0      0  \n",
       "664        0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate dataframe into X and Y for model\n",
    "Y = pd.DataFrame(data['label'])\n",
    "X = data.drop(columns=['label'])\n",
    "\n",
    "# data is split into train/validation and test sets\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(X, Y, \n",
    "                                                    test_size=0.20, \n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42 #seed\n",
    "                                                    )\n",
    "\n",
    "x_train_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature amount: 784\n",
      "Features with a variance greater than 0.01: 710\n"
     ]
    }
   ],
   "source": [
    "sel = VarianceThreshold(threshold=0.01)\n",
    "sel.fit(x_train_val)  # fit finds the features with zero variance\n",
    "\n",
    "print(f'Feature amount: {x_train_val.shape[1]}')\n",
    "print(f'Features with a variance greater than 0.01: {sum(sel.get_support())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (56000, 710)\n",
      "Test shape: (14000, 710)\n"
     ]
    }
   ],
   "source": [
    "x_train_val = sel.transform(x_train_val)\n",
    "x_test = sel.transform(x_test)\n",
    "\n",
    "print(f'Train shape: {x_train_val.shape}')\n",
    "print(f'Test shape: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since the model selection process will require a validation set for hyper-parameter tuning, we will divide the train set into training and validation. Currently, our train set contains 80% of the dataset. We will divide the set so that train contains 60% and validation contains 20% of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (42000, 710)\n",
      "Test shape: (14000, 710)\n"
     ]
    }
   ],
   "source": [
    "# data is split into train and validation sets\n",
    "# shuffle is skipped, since the dataset had already been shuffled when separating into training and test\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.25)\n",
    "y_train = y_train['label'] # changes y_train to a Series\n",
    "\n",
    "print(f'Train shape: {x_train.shape}')\n",
    "print(f'Test shape: {x_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "With our dataset already preprocessed, we can now design an experiment for selecting and tuning our model. The experimental process consists of the following steps:\n",
    "1. Defining 3 possible models\n",
    "2. For each model:\n",
    "    1. Define possible values for selected hyperparameters\n",
    "    2. Validate each hyperparameter and calculate the metrics for each one\n",
    "    3. Select hyperparameters with the highest metrics\n",
    "3. Execute each model with the calculated hyperparameters\n",
    "4. Calculate Metrics\n",
    "5. Graph results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Models\n",
    "\n",
    "In this section, we will explain each of the three selected algorithms, as well as their respective implementation. Each of these models will be executed using their corresponding Scikit-learn implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbours\n",
    "This model works both for regression and for classification purposes. It follows a simple principle: birds of a feather flock together. In other words, it assumes that similar data entries are close together. For classification, this means that when graphing the dataset, we can see that rows with the same category are closer together than rows with different categories. \n",
    "\n",
    "When given a value with an unkwnown label, the algorithm calculates and stores the distance between the unknown value and each of the known values of the dataset. Once calculated, these distances are ordered from shortest to longest. Then, the K closer values (nearest neighbours) are chosen, and the mode of their classes is calculated. This class is then assigned to the queried value. It is a good idea to use an odd K, since this would mean that a tie between the most common classes is impossible. If an even number was chosen and a tie was found, it would have to be broken choosing one of the tied classes at random.\n",
    "\n",
    "This algorithm can be executed using the KNeighborsClassifier class from sklearn.neighbors. Below, we provide a detailed description of its implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNeighborsClassifier**(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\n",
    "Parameters:\n",
    "* **n_neighbors**: value of K, that is, the selected number of neighbors to use for the class assignment\n",
    "* **weights**: way in which distances are taken into account\n",
    "* **algorithm**: approach taken to calculate the nearest neighbors\n",
    "* **leaf_size**: hyperparameter needed by some of the possible algorithms\n",
    "* **p**: power parameter used for some of the metrics\n",
    "* **metric**: way in which distances between values are calculated\n",
    "* **metric_params**: dictionary for passing additional parameters a metric might need\n",
    "* **n_jobs**: number of jobs to be run in parallel in order to speed up the nearest neighbor calculation\n",
    "\n",
    "More information can be found in the official documentation:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "In order to understand Random Forest, we must first understand Decision Trees, which are the foundation for the model. These trees take a given dataset, and build a tree for it, where all of the leaf nodes are values for the different classes present in the dataset. The root of the tree represents one of the dataset's features, and it has a branch for each of the values found for that feature in the dataset. These branches can either end up in a leaf node or lead to a child node which represents another feature. These child nodes, like the root node, have a branch for each of the values found for it in the dataset.\n",
    "\n",
    "When given a new unlabeled row, one could take the values of each feature and go through the tree by choosing the branch corresponding to this row's features (hence the name, decision trees). \n",
    "\n",
    "Decision trees are built by using heuristic criteria such as Information Gain to choose which features should be closer to the root of the tree. However, these trees can suffer from overfitting. This is where Random Forest comes in.\n",
    "\n",
    "Random forest creates many trees (it's a forest after all) and randomizes their creation process. This can be done by dividing the dataset in subsets and creating a decision tree for each one, or by dividing features into groups and creating trees for each group. Any unlabeled row is processed with all of the trees in the forest, and consensus regarding the new row's class is reached by using all of the different results from the trees.\n",
    "\n",
    "This algorithm can be executed using the RandomForestClassifier class from sklearn.ensemble. More information about this implementation is included below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RandomForestClassifier**(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "* **n_estimators**: number of trees in the forest\n",
    "* **criterion**: heuristic criterion for decision tree building\n",
    "* **max_depth**: maximum depth each tree can take\n",
    "* **min_samples_split**: minimum number of observations needed to create a branch for a given value\n",
    "* **min_samples_leaf**: minimum number of observations needed to create a leaf node from a given value\n",
    "* **min_weight_fraction_leaf**: same as sample leaf, but adds a weighing factor to values\n",
    "* **max_features**: number of features to consider when creating a tree\n",
    "* **max_leaf_nodes**: maximum number of leaf nodes allowed\n",
    "* **min_impurity_decrease**: additional (and optional) criterion for tree building process\n",
    "* **bootstrap**: determines if the whole dataset is used to build each tree, or if it is bootstrapped\n",
    "* **oob_score**: parameter needed if bootstrapping is enabled\n",
    "* **n_jobs**: number of jobs to be run in parallel in order to speed up the process\n",
    "* **random_state**: seed for random factors of the algorithm (such as bootstrapping)\n",
    "* **verbose**: amount of detail provided when fitting and predicting\n",
    "* **warm_start**: if true, it uses the trees from a previous fit as well\n",
    "* **class_weight**: weights associated with each class in the dataset\n",
    "* **ccp_alpha**: criterion needed if pruning was performed to be performed to the tree\n",
    "* **max_samples**: number of samples to draw from dataset for training if bootstrapping is being used\n",
    "\n",
    "The documentation provides further information if needed: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Discriminant\n",
    "\n",
    "Fisher's Linear Discriminant approaches a classification problem using dimensionality reduction (such as SVD or eigenvalue decomposition). This model reduces the dimensionality of the data to one, in order to simplify the distinction of the different classes. \n",
    "\n",
    "However, when dimension is reduced, data is lost. This means that classes can end up appearing to be closer together (when reduced to one dimension) than what they really are. Hence, this model aims to maximize the variance between different classes (inter-class variance) and minimize variance inside each class (intra-class variance) when the reduction is performed.\n",
    "\n",
    "This model can be LinearDiscriminantAnalysis from sklearn.discriminant_analysis. A description of said model can be found below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LinearDiscriminantAnalysis**(solver='svd', shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001, covariance_estimator=None)\n",
    "\n",
    "* **solver**: method for dimensionality reduction\n",
    "* **shrinkage**: additional parameter needed for some of the solvers\n",
    "* **priors**: info about different classes. Can be inferred\n",
    "* **n_components**: number of components for dimensionality reduction\n",
    "* **store_covariance**: determines if intra class variance matrix should be stored if solver is svd\n",
    "* **tol**: hyperparameter needed if solver is svd\n",
    "* **covariance_estimator**: used to provide a different covariance calculation method\n",
    "\n",
    "As with the previous models, more information can be found in the official documentation: https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Selection\n",
    "\n",
    "Now that we have a better understanding of the chosen models, we will tune each of our models, selecting the best hyperparameters found. This will be done selecting a range of possible values for each hyperparameter and fitting each model with the training set, once for each possible hyperparameter value. Then, each of these fitted models will be evaluated with the validation set, and some metrics will be calculated for them. The hyperparameter combination with the highest metrics will be chosen for the next stage of this experimental process.\n",
    "\n",
    "The hyperparameters that will be tested are:\n",
    "* **KNeighbors**: n_neighbors\n",
    "* **RandomForest**: n_estimators, criterion\n",
    "* **LinearDiscriminantAnalysis**: solver\n",
    "\n",
    "The metrics that will be used to select the hyperparameters are:\n",
    "* **Accuracy**: measures how close our predictions where to the real values.\n",
    "* **F1-score**: combines both precision and recall metrics.\n",
    "    * Precision: $\\frac{True Positives}{True Positives + False Positives}$ \n",
    "    * Recall:$\\frac{True Positives}{True Positives + False Negatives}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we define our possible hyperparameters\n",
    "\n",
    "# KNN - notice that odd number of neighbors are chosen in order to avoid ties\n",
    "neighbors = [i for i in range(1,80,2)]\n",
    "\n",
    "# Random Forest\n",
    "trees = [i for i in range(100, 300, 20)]\n",
    "criteria = ['gini', 'entropy']\n",
    "\n",
    "# Linear discriminant\n",
    "solvers = ['svd', 'lsqr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we create a structure to store the metrics we are about to calculate\n",
    "# Each inner dictionary will have a key for each hyperparemeter combination, and an array with the metrics\n",
    "metrics = {'KNN':{}, 'Random Forest':{}, 'Linear Discriminant': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we train our model for each of the specified hyperparameters\n",
    "# and we validate them in order to calculate the desired metrics\n",
    "\n",
    "# KNN\n",
    "for n in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors = n)\n",
    "    knn.fit(x_train, y_train)\n",
    "    y_pred = knn.predict(x_val)\n",
    "    metrics['KNN'][str(n)] = [accuracy_score(y_val, y_pred), f1_score(y_val, y_pred, average='macro')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "for t in trees:\n",
    "    for c in criteria:\n",
    "        rf = RandomForestClassifier(n_estimators=t, criterion=c)\n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_val)\n",
    "        key = c + str(t)\n",
    "        metrics['Random Forest'][key] = [accuracy_score(y_val, y_pred), f1_score(y_val, y_pred, average='macro')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant\n",
    "for s in solvers:\n",
    "    ld = LinearDiscriminantAnalysis(solver=s)\n",
    "    ld.fit(x_train, y_train)\n",
    "    y_pred = ld.predict(x_val)\n",
    "    metrics['Linear Discriminant'][s] = [accuracy_score(y_val, y_pred), f1_score(y_val, y_pred, average='macro')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "  * accuracy: 1 = 0.969\n",
      "  * f1-score: 1 = 0.968646606734777\n",
      "\n",
      "Random Forest\n",
      "  * accuracy: gini750 = 0.9687857142857143\n",
      "  * f1-score: gini750 = 0.9684315616558609\n",
      "\n",
      "Linear Discriminant\n",
      "  * accuracy: lsqr = 0.8637142857142858\n",
      "  * f1-score: lsqr = 0.8618810074942136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With all of the metrics calculated, we can now select the best combinations of hyperparameters\n",
    "\n",
    "# function to get the hyperparameter combination with the highest metrics\n",
    "def best_metrics(metrics_dict):\n",
    "    best_acc = ''\n",
    "    best_acc_val = 0\n",
    "    best_f1 = ''\n",
    "    best_f1_val = 0\n",
    "    \n",
    "    for k in metrics_dict.keys():\n",
    "        mets = metrics_dict[k]\n",
    "        if mets[0] > best_acc_val:\n",
    "            best_acc = k\n",
    "            best_acc_val = mets[0]\n",
    "            \n",
    "        if mets[1] > best_f1_val:\n",
    "            best_f1 = k\n",
    "            best_f1_val = mets[1]\n",
    "        \n",
    "    return best_acc, best_acc_val, best_f1, best_f1_val\n",
    "    \n",
    "\n",
    "for i in metrics.keys():\n",
    "    print(i)\n",
    "    acc, acc_v, f1, f1_v = best_metrics(metrics[i])\n",
    "    print(f'  * accuracy: {acc} = {acc_v}')\n",
    "    print(f'  * f1-score: {f1} = {f1_v}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, we can see that the best hyperparameters are:\n",
    "* KNN\n",
    "    * n_neighbors = 1\n",
    "* Random Forest\n",
    "    * criterion = gini\n",
    "    * n_estimators = 750\n",
    "* Linear Discriminant\n",
    "    * solver = lsqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison\n",
    "\n",
    "In the previous section, we tuned the hyperparameters for our models. In other words, we chose the most appropriate hyperparameter values for our models. We can now use these values to train our models and determine which one yields the best results for our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the 3 models to be tested with the calculated hyperparameters\n",
    "models = {'KNN' : KNeighborsClassifier(n_neighbors=1),\n",
    "         'Random Forest' : RandomForestClassifier(n_estimators=750, criterion='gini'),\n",
    "         'Linear Discriminant': LinearDiscriminantAnalysis(solver='lsqr')}\n",
    "\n",
    "# dictionary for storing final metrics\n",
    "final_metrics = {'KNN': [], 'Random Forest': [], 'Linear Discriminant': []}\n",
    "\n",
    "# for each model, we fit the data and then test it's predictions with the test set\n",
    "for k in models.keys():\n",
    "    model = models[k]\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    final_metrics[k] = [accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='macro')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "  * accuracy: 0.9695714285714285\n",
      "  * f1-score: 0.9694176242056318\n",
      "\n",
      "Random Forest\n",
      "  * accuracy: 0.9659285714285715\n",
      "  * f1-score: 0.9657096008321858\n",
      "\n",
      "Linear Discriminant\n",
      "  * accuracy: 0.8674285714285714\n",
      "  * f1-score: 0.866447630234058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in final_metrics:\n",
    "    print(model)\n",
    "    acc, f1 = final_metrics[model]\n",
    "    print(f'  * accuracy: {acc}')\n",
    "    print(f'  * f1-score: {f1}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, we can see that all models have a high score both for accuracy and f1. The Linear Discriminant model was the worst of the tree, and KNN and Random Forest where closer together.\n",
    "\n",
    "The difference between KNN and Random Forest was approximately 0.004 for both models. However, KNN performed slightly better. Furthermore, the KNN model was faster to run than Random Forest, which adds another reason for prefering said model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Model\n",
    "\n",
    "Based on the previous results, we recommend using a K Nearest Neighbors algorithm, using a value of K = 1. All other hyperparameters were left with the default values provided by scikit-learn.\n",
    "\n",
    "We recommend this model due to its high accuracy, which means that our model manages presents very few false positive and false negative cases. This model also achieved a remarkably high f1-score, which means that it is precise and it has a high recall rate.\n",
    "\n",
    "Also, the model is easy to understand and is quick to run (due to the small value of K). These reasons constitute added bonuses for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Parte 2  -  Clustering  (50 puntos)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considere lo siguiente:\n",
    "1. Conjunto de datos: tarjetas.csv\n",
    "1. Este dataset corresponde a datos de tarjetas de crédito, a partir de los cuales se desea descubrir cuántos grupos (clusters) se podrían encontrar.\n",
    "1. Proponga al menos dos algoritmos a utilizar\n",
    "1. Proponga una métrica, la cual debe ser **interna**.\n",
    "1. Debe presentar en este cuaderno:\n",
    "    1. Pre-procesamiento de los datos, explicando las decisiones en cada caso. Observará que en este caso viene bastante preparado\n",
    "    1. Para cada uno de los algoritmos seleccionados: \n",
    "        1. Explicación del algoritmo.\n",
    "        1. Explicación de la implementación seleccionada y de sus parámetros.\n",
    "    1. Explicación del diseño experimental por ejecutar. En este caso recuerde que es aprendizaje NO Supervisado. Además del númeoro de clusters, se le recomienda confirmar con el profesor cuántos y cuáles hyperparámetros validar. \n",
    "    1. Programación del diseño experimental. \n",
    "    1. Muestre en una tabla los resultados de la métrica.\n",
    "    1. Grafique las métrica para ver el criterio del codo.\n",
    "    1. ¿Cuál es el númermo de cluster que considera mejor describe el conjunto de datos? ¿Cómo defendería su recomendación ante su contratante? (Considere en su respuesta los algoritmos que utilizó)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Evaluación_01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
